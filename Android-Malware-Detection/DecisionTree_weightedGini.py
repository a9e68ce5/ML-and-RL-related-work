import numpy as np
import pandas as pd

# -------------------------------
# DecisionTree 類別（含樣本重加權功能）
# -------------------------------
#實作一個簡單的決策樹，並在建樹過程中加入了樣本重加權功能，以處理類別不平衡的問題。
class DecisionTree:
    def __init__(self, max_depth=20, min_samples_split=2):
        self.max_depth = max_depth #限制樹的最大深度，防止樹過於複雜。
        self.min_samples_split = min_samples_split #若節點的樣本數少於此值，則不再繼續分裂。
        self.tree = None #存放訓練完成後的決策樹結構。

    def fit(self, X, y, sample_weights=None):
        """
        訓練決策樹模型。
        若未傳入 sample_weights，則所有樣本預設權重均為 1。
        """
        if sample_weights is None:  #傳入的 X 為特徵矩陣，y 為標籤，sample_weights 為每個樣本的權重（若沒有指定，則預設每個樣本權重皆為 1）。
            sample_weights = np.ones(len(y))
        self.tree = self._build_tree(X, y, sample_weights, depth=0)

    def _gini(self, y, weights):
        """計算加權 Gini impurity"""
        total_weight = np.sum(weights)
        impurity = 1.0
        for label in np.unique(y):
            label_weight = np.sum(weights[y == label]) #先計算總權重，再依據各標籤的權重比例計算不純度。
            p = label_weight / total_weight
            impurity -= p ** 2
        return impurity

    def _build_tree(self, X, y, weights, depth):
        # 停止條件：所有樣本同屬一類、當樣本數量不足以進行分裂或已達最大深度
        if len(np.unique(y)) == 1 or len(y) < self.min_samples_split or depth >= self.max_depth: 
            return {'leaf': True, 'prediction': self._weighted_majority_class(y, weights)}
        
        best_impurity = float('inf')
        best_feature = None
        best_threshold = None
        best_left_idx = None
        best_right_idx = None

        n_samples, n_features = X.shape

        # 對每個特徵的每個唯一值嘗試作為分割閥值
        # 嘗試對每個特徵及其所有唯一值作為分割閥值，計算分割後左右子集的加權 Gini impurity，並選擇使 impurity 最小的那個分割點
        for feature in range(n_features):
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                left_idx = np.where(X[:, feature] <= threshold)[0]
                right_idx = np.where(X[:, feature] > threshold)[0]
                if len(left_idx) == 0 or len(right_idx) == 0:
                    continue
                left_weight = np.sum(weights[left_idx])
                right_weight = np.sum(weights[right_idx])
                total_weight = left_weight + right_weight

                gini_left = self._gini(y[left_idx], weights[left_idx])
                gini_right = self._gini(y[right_idx], weights[right_idx])
                weighted_impurity = (left_weight / total_weight) * gini_left + (right_weight / total_weight) * gini_right

                if weighted_impurity < best_impurity:
                    best_impurity = weighted_impurity
                    best_feature = feature
                    best_threshold = threshold
                    best_left_idx = left_idx
                    best_right_idx = right_idx

        if best_feature is None:
            return {'leaf': True, 'prediction': self._weighted_majority_class(y, weights)}
        
        # 遞迴建立左右子樹
        left_tree = self._build_tree(X[best_left_idx], y[best_left_idx], weights[best_left_idx], depth + 1)
        right_tree = self._build_tree(X[best_right_idx], y[best_right_idx], weights[best_right_idx], depth + 1)

        return {
            'leaf': False,
            'feature': best_feature,
            'threshold': best_threshold,
            'left': left_tree,
            'right': right_tree
        }
    #對於一個節點中的所有樣本，計算每個類別的加權總和，選出總權重最大的類別作為該葉節點的預測結果。
    def _weighted_majority_class(self, y, weights):
        """以加權多數投票決定葉節點的預測類別"""
        classes = np.unique(y)
        best_class = None
        best_weight = -1
        for label in classes:
            total = np.sum(weights[y == label])
            if total > best_weight:
                best_weight = total
                best_class = label
        return best_class
    #從根節點開始，根據樣本 x 的特徵值與節點中記錄的分割閥值進行比較，沿著左子樹或右子樹遞迴尋找最終的葉節點，並返回該葉節點的預測結果。
    #為什麼使用加權 Gini impurity
    #處理類別不平衡：
    #當某個類別在數據中佔絕大多數時，傳統 Gini impurity 可能會因為數量上的壓倒性優勢而讓模型傾向預測該類別。透過加權，可以給較少數的類別更高的權重，讓模型在分裂時更加重視這部分資料。

    #考慮樣本重要性：
    #如果某些樣本因為其來源或其他因素具有更高的重要性，可以給予較大的權重。這樣在計算 impurity 時，這些樣本的影響會更大，進而使決策樹在分割時更能反映這些重要樣本的分布。

    #更精細的分割：
    #透過引入權重，決策樹在選擇分割點時不僅僅依賴樣本數量，而是考慮樣本的重要性或代表性，從而有助於提高模型的泛化能力。
    def predict_single(self, x, node):
        """對單一樣本進行預測（遞迴遍歷樹）"""
        if node['leaf']:
            return node['prediction']
        if x[node['feature']] <= node['threshold']:
            return self.predict_single(x, node['left'])
        else:
            return self.predict_single(x, node['right'])

    def predict(self, X):
        """對多個樣本進行預測"""
        return np.array([self.predict_single(x, self.tree) for x in X])

# -------------------------------
# 輔助函數：從 DataFrame 中抽取特徵欄位（以 x 為開頭）
# -------------------------------
def extract_features(df):
    feature_cols = [col for col in df.columns if col.startswith('x')]
    return df[feature_cols].values

# -------------------------------
# 主程式：讀取資料、訓練模型、產生預測與提交檔案
# -------------------------------
if __name__ == "__main__":
    # 讀取訓練資料
    train_df = pd.read_csv("./data/train.csv")
    # 目標欄位名稱固定為 label，其他以 x 開頭的欄位皆為特徵
    y_train = train_df['label'].values
    X_train = extract_features(train_df)
    
    # 計算類別權重： weight = 總樣本數 / (類別數 * 該類別樣本數)
    unique, counts = np.unique(y_train, return_counts=True)
    total_samples = len(y_train)
    n_classes = len(unique)
    class_weights = {label: total_samples / (n_classes * count) for label, count in zip(unique, counts)}
    # 為每個訓練樣本指定對應權重
    sample_weights = np.array([class_weights[label] for label in y_train])
    
    # 初始化並訓練決策樹模型（此處 max_depth 可依需求調整）
    tree = DecisionTree(max_depth=10, min_samples_split=2)
    tree.fit(X_train, y_train, sample_weights=sample_weights)
    
    # -------------------------------
    # (1) 測試集預測：讀取 data/test.csv
    # -------------------------------
    test_df = pd.read_csv("./data/test.csv")
    # 若 test.csv 中有 label 可用來評估模型效能
    if 'label' in test_df.columns:
        y_test = test_df['label'].values
    X_test = extract_features(test_df)
    test_predictions = tree.predict(X_test)
    print("Test Predictions:")
    print(test_predictions)
    
    if 'label' in test_df.columns:
        accuracy = np.mean(test_predictions == y_test)
        print("Test Accuracy:", accuracy)
    
    # -------------------------------
    # (2) Kaggle 提交預測：讀取 data/eval.anon.csv 與 data/eval.ids
    # -------------------------------
    eval_df = pd.read_csv("./data/eval.anon.csv")
    X_eval = extract_features(eval_df)
    eval_predictions = tree.predict(X_eval)
    
    # 讀取 evaluation ids（無表頭，每行為一個 id）
    eval_ids_df = pd.read_csv("./data/eval.id", header=None, names=["id"])
    # 檢查 id 數量與預測數量是否一致
    assert len(eval_ids_df) == len(eval_predictions), "eval.ids 與 eval.anon.csv 的資料筆數不相符"
    
    # 組合 id 與預測結果，依規定格式產生提交檔案
    submission_df = pd.DataFrame({
        "example_id": eval_ids_df["id"],
        "label": eval_predictions
    })
    submission_df.to_csv("submission.csv", index=False)
    print("Submission file saved as submission.csv")
