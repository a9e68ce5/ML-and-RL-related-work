import numpy as np
import pandas as pd
import os
from cvxopt import matrix, solvers
import matplotlib.pyplot as plt

# -------------------------------
# 1. 資料讀取與前處理
# -------------------------------
def load_data(train_file, test_file):
    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)
    # 假設標籤欄位名稱為 "label"，其他以 x 開頭的欄位為特徵
    X_train = train_df.filter(regex="^x").values
    y_train = train_df['label'].values
    X_test = test_df.filter(regex="^x").values
    y_test = test_df['label'].values
    return X_train, y_train, X_test, y_test

def standardize_data(X, mean=None, std=None):
    if mean is None or std is None:
        mean = np.mean(X, axis=0)
        std = np.std(X, axis=0)
    std[std == 0] = 1
    X_std = (X - mean) / std
    return X_std, mean, std

# -------------------------------
# 2. PCA 實作：根據累積解釋變異（例如 80%）決定保留主成分數目
# -------------------------------
def perform_pca(X, variance_threshold=0.8):
    mean = np.mean(X, axis=0)
    X_centered = X - mean
    cov_matrix = np.cov(X_centered, rowvar=False)
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
    sorted_idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sorted_idx]
    eigenvectors = eigenvectors[:, sorted_idx]
    explained_variances = eigenvalues / np.sum(eigenvalues)
    cumulative_variance = np.cumsum(explained_variances)
    k = np.searchsorted(cumulative_variance, variance_threshold) + 1
    eigenvectors_reduced = eigenvectors[:, :k]
    X_reduced = np.dot(X_centered, eigenvectors_reduced)
    return X_reduced, eigenvectors_reduced, mean, k, cumulative_variance

# -------------------------------
# 3. 定義 RBF kernel 與 SVM 相關函式
# -------------------------------
def rbf_kernel(X1, X2, gamma):
    X1_sq = np.sum(X1**2, axis=1).reshape(-1, 1)
    X2_sq = np.sum(X2**2, axis=1).reshape(1, -1)
    dist_sq = X1_sq + X2_sq - 2 * np.dot(X1, X2.T)
    dist_sq = np.maximum(dist_sq, 0)
    return np.exp(-gamma * dist_sq)

def svm_train(X, y, C, kernel_func, gamma, tol=1e-4):
    n_samples = X.shape[0]
    K = kernel_func(X, X, gamma)
    P = matrix(np.outer(y, y) * K)
    q = matrix(-np.ones(n_samples))
    G_std = np.diag(-np.ones(n_samples))
    h_std = np.zeros(n_samples)
    G_slack = np.diag(np.ones(n_samples))
    h_slack = np.ones(n_samples) * C
    G = matrix(np.vstack((G_std, G_slack)))
    h = matrix(np.hstack((h_std, h_slack)))
    A = matrix(y.reshape(1, -1))
    b = matrix(np.zeros(1))
    solvers.options['show_progress'] = False
    solution = solvers.qp(P, q, G, h, A, b)
    alphas = np.ravel(solution['x'])
    sv = alphas > tol
    ind = np.arange(len(alphas))[sv]
    alphas_sv = alphas[sv]
    sv_X = X[sv]
    sv_y = y[sv]
    b_val = 0
    for n in range(len(alphas_sv)):
        b_val += sv_y[n] - np.sum(alphas_sv * sv_y * K[ind[n], sv])
    b_val /= len(alphas_sv)
    model = {
        'alphas': alphas_sv,
        'sv_X': sv_X,
        'sv_y': sv_y,
        'b': b_val,
        'gamma': gamma,
        'kernel_func': kernel_func
    }
    return model

def svm_decision_function(model, X):
    alphas = model['alphas']
    sv_X = model['sv_X']
    sv_y = model['sv_y']
    b_val = model['b']
    gamma = model['gamma']
    kernel_func = model['kernel_func']
    K = kernel_func(X, sv_X, gamma)
    decision_values = np.dot(K, alphas * sv_y) + b_val
    return decision_values

def svm_predict(model, X, use_calibration=False, threshold=0.5):
    decision_values = svm_decision_function(model, X)
    if use_calibration and 'calibration_method' in model:
        # 如果是 isotonic 校準
        if model['calibration_method'] == 'isotonic':
            f_sorted, fitted = model['calibration']
            prob = np.interp(decision_values, f_sorted, fitted)
        else:
            # 預設: Platt scaling
            A, B = model['calibration']
            prob = 1.0 / (1.0 + np.exp(A * decision_values + B))
        predictions = np.where(prob >= threshold, 1, -1)
        return predictions, prob
    else:
        predictions = np.sign(decision_values)
        return predictions, decision_values

def accuracy_score(y_true, y_pred):
    return np.mean(y_true == y_pred)

# -------------------------------
# 4. Isotonic Regression 實作 (PAVA) & 校準
# -------------------------------
def isotonic_regression_PAVA(y):
    n = len(y)
    blocks = []
    for i in range(n):
        blocks.append([y[i], 1, i, i])
        while len(blocks) > 1 and (blocks[-2][0]/blocks[-2][1]) > (blocks[-1][0]/blocks[-1][1]):
            b2 = blocks.pop()
            b1 = blocks.pop()
            new_sum = b1[0] + b2[0]
            new_weight = b1[1] + b2[1]
            blocks.append([new_sum, new_weight, b1[2], b2[3]])
    fitted = np.empty(n)
    for block in blocks:
        avg = block[0] / block[1]
        for i in range(block[2], block[3] + 1):
            fitted[i] = avg
    return fitted

def calibrate_model_isotonic(model, X_calib, y_calib):
    decision_values = svm_decision_function(model, X_calib)
    y_calib_binary = np.where(y_calib == -1, 0, 1)
    order = np.argsort(decision_values)
    f_sorted = decision_values[order]
    y_sorted = y_calib_binary[order]
    fitted = isotonic_regression_PAVA(y_sorted)
    model['calibration_method'] = 'isotonic'
    model['calibration'] = (f_sorted, fitted)
    return model

# -------------------------------
# 5. 自動選擇最佳閾值函式
# -------------------------------
def find_optimal_threshold(probs, labels):
    thresholds = np.linspace(0, 1, 101)
    best_thresh = 0.5
    best_acc = 0
    for thresh in thresholds:
        preds = np.where(probs >= thresh, 1, -1)
        acc = np.mean(preds == labels)
        if acc > best_acc:
            best_acc = acc
            best_thresh = thresh
    return best_thresh, best_acc

# -------------------------------
# 6. 資料分布分析函式
# -------------------------------
def analyze_distribution(X_train, X_test, X_eval):
    stats = {
        'train_mean': np.mean(X_train, axis=0),
        'test_mean': np.mean(X_test, axis=0),
        'eval_mean': np.mean(X_eval, axis=0),
        'train_std': np.std(X_train, axis=0),
        'test_std': np.std(X_test, axis=0),
        'eval_std': np.std(X_eval, axis=0)
    }
    return stats

# -------------------------------
# 7. 繪製校準曲線
# -------------------------------
def plot_calibration_curve(probs, true_labels, n_bins=10):
    bin_edges = np.linspace(0, 1, n_bins + 1)
    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2
    bin_true = np.zeros(n_bins)
    bin_pred = np.zeros(n_bins)
    bin_counts = np.zeros(n_bins)
    
    for p, t in zip(probs, true_labels):
        bin_idx = np.digitize(p, bin_edges, right=True) - 1
        bin_idx = np.clip(bin_idx, 0, n_bins - 1)
        bin_true[bin_idx] += t
        bin_pred[bin_idx] += p
        bin_counts[bin_idx] += 1
        
    nonzero = bin_counts > 0
    avg_true = np.zeros(n_bins)
    avg_pred = np.zeros(n_bins)
    avg_true[nonzero] = bin_true[nonzero] / bin_counts[nonzero]
    avg_pred[nonzero] = bin_pred[nonzero] / bin_counts[nonzero]
    
    plt.figure(figsize=(8, 6))
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')
    plt.plot(avg_pred, avg_true, marker='o', linewidth=2, label='Calibration Curve')
    plt.xlabel('Mean Predicted Probability')
    plt.ylabel('Fraction of Positives')
    plt.title('Calibration Curve')
    plt.legend()
    plt.show()

# -------------------------------
# 8. Hold-out 驗證函式 (對 C, gamma 進行網格搜尋)
# -------------------------------
def holdout_validation(X, y, c_values, gamma_values, validation_ratio=0.2, variance_threshold=0.8, tol=1e-4):
    """
    在 (C, gamma) 的組合上進行搜尋，回傳 (best_c, best_gamma) 與所有結果。
    """
    n_samples = X.shape[0]
    indices = np.arange(n_samples)
    np.random.shuffle(indices)
    split = int(n_samples * (1 - validation_ratio))
    train_idx = indices[:split]
    val_idx = indices[split:]
    
    X_train_hold = X[train_idx]
    y_train_hold = y[train_idx]
    X_val_hold = X[val_idx]
    y_val_hold = y[val_idx]
    
    # 標準化與 PCA（以 hold-out 訓練集為基準）
    X_train_hold_std, hold_mean, hold_std = standardize_data(X_train_hold)
    X_val_hold_std, _, _ = standardize_data(X_val_hold, mean=hold_mean, std=hold_std)
    
    X_train_hold_pca, hold_pca_components, hold_pca_mean, k, _ = perform_pca(X_train_hold_std, variance_threshold=variance_threshold)
    X_val_hold_centered = X_val_hold_std - hold_pca_mean
    X_val_hold_pca = np.dot(X_val_hold_centered, hold_pca_components)
    
    results = {}
    for C in c_values:
        for gamma in gamma_values:
            model = svm_train(X_train_hold_pca, y_train_hold, C, rbf_kernel, gamma, tol)
            y_val_pred, _ = svm_predict(model, X_val_hold_pca)
            acc = accuracy_score(y_val_hold, y_val_pred)
            results[(C, gamma)] = acc
    
    # 取得 (C, gamma) 對應最大驗證集準確率
    best_combo = max(results, key=lambda cg: results[cg])  # (best_c, best_gamma)
    return best_combo, results

# -------------------------------
# 9. 主流程
# -------------------------------
def main():
    train_file = 'data/train.csv'
    test_file = 'data/test.csv'
    
    print("讀取訓練與測試資料...")
    X_train, y_train, X_test, y_test = load_data(train_file, test_file)
    
    # 轉換標籤 (0 -> -1)
    y_train = np.where(y_train == 0, -1, 1).astype(np.float64)
    y_test = np.where(y_test == 0, -1, 1).astype(np.float64)
    
    print("對訓練資料進行標準化...")
    X_train_std, train_mean, train_std = standardize_data(X_train)
    X_test_std, _, _ = standardize_data(X_test, mean=train_mean, std=train_std)
    
    print("對訓練資料執行 PCA ...")
    X_train_pca, pca_components, pca_mean, k, cumulative_variance = perform_pca(X_train_std, variance_threshold=0.99)
    print(f"PCA 完成，保留 {k} 個主成分")
    
    X_test_centered = X_test_std - pca_mean
    X_test_pca = np.dot(X_test_centered, pca_components)
    
    print("\n進行資料分布分析 (PCA 空間)：")
    stats = analyze_distribution(X_train_pca, X_test_pca, X_test_pca)
    print("訓練資料平均值：", stats['train_mean'])
    print("測試資料平均值：", stats['test_mean'])
    
    # -------------------------------
    # 使用 Hold-out 驗證搜尋 (C, gamma)
    # -------------------------------
    c_values = [0.1, 1, 10, 100]
    gamma_values = [0.01, 0.05, 0.1, 0.5, 1, 5, 10]
    
    print("\n開始進行 (C, gamma) 網格搜尋 (Hold-out 驗證)...")
    best_combo, holdout_results = holdout_validation(X_train, y_train, c_values, gamma_values,
                                                     validation_ratio=0.2, variance_threshold=0.9, tol=1e-4)
    best_c, best_gamma = best_combo
    print("各組合驗證集準確率:")
    for combo, acc in holdout_results.items():
        print(f"  C={combo[0]}, gamma={combo[1]} -> {acc*100:.2f}%")
    print(f"\nHold-out 驗證選出的最佳 (C, gamma) = ({best_c}, {best_gamma})")
    
    # -------------------------------
    # 使用最佳參數 重新訓練完整模型
    # -------------------------------
    print(f"\n使用最佳 (C={best_c}, gamma={best_gamma}) 重新訓練模型 ...")
    best_model = svm_train(X_train_pca, y_train, best_c, rbf_kernel, best_gamma)
    
    y_train_pred, _ = svm_predict(best_model, X_train_pca)
    train_acc = accuracy_score(y_train, y_train_pred)
    y_test_pred, _ = svm_predict(best_model, X_test_pca)
    test_acc = accuracy_score(y_test, y_test_pred)
    print(f"完整訓練集準確率: {train_acc*100:.2f}%，測試集準確率: {test_acc*100:.2f}%")
    
    # -------------------------------
    # 利用測試集進行 Isotonic 校準
    # -------------------------------
    best_model = calibrate_model_isotonic(best_model, X_test_pca, y_test)
    print("Isotonic 校準完成。")
    
    # 在測試集上搜尋最佳閾值
    _, test_probs = svm_predict(best_model, X_test_pca, use_calibration=True)
    optimal_thresh, opt_acc = find_optimal_threshold(test_probs, y_test)
    print(f"從測試集選出的最佳閾值為 {optimal_thresh:.2f} (準確率: {opt_acc*100:.2f}%)")
    
    # 繪製測試集的校準曲線
    y_test_binary = np.where(y_test == -1, 0, 1)
    plot_calibration_curve(test_probs, y_test_binary, n_bins=10)
    
    # -------------------------------
    # 讀取 evaluation 資料並進行預測
    # -------------------------------
    print("\n讀取 evaluation 資料 ...")
    eval_df = pd.read_csv("data/eval.anon.csv")
    eval_ids = pd.read_csv("data/eval.id", header=None, names=["id"])
    
    X_eval = eval_df.filter(regex="^x").values
    X_eval_std, _, _ = standardize_data(X_eval, mean=train_mean, std=train_std)
    X_eval_centered = X_eval_std - pca_mean
    X_eval_pca = np.dot(X_eval_centered, pca_components)
    
    eval_stats = analyze_distribution(X_train_pca, X_test_pca, X_eval_pca)
    print("Evaluation 資料平均值：", eval_stats['eval_mean'])
    
    # 使用校準後模型與最佳閾值預測 evaluation 資料
    eval_pred, eval_prob = svm_predict(best_model, X_eval_pca, use_calibration=True, threshold=optimal_thresh)
    eval_pred_converted = np.where(eval_pred == -1, 0, 1)
    
    output_df = pd.DataFrame({"example_id": eval_ids["id"], "label": eval_pred_converted})
    output_folder = "SVM"
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    output_path = os.path.join(output_folder, "svm_eval_predictions_ho.csv")
    output_df.to_csv(output_path, index=False)
    print(f"Evaluation 預測結果已寫入：{output_path}")

if __name__ == "__main__":
    main()
