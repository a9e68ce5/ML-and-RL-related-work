import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from cvxopt import matrix, solvers
import random
from scipy.optimize import minimize

###############################################################################
# 1. 資料讀取、標準化、PCA 等
###############################################################################
def load_data(train_file, test_file):
    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)
    X_train = train_df.filter(regex="^x").values
    y_train = train_df['label'].values
    X_test = test_df.filter(regex="^x").values
    y_test = test_df['label'].values
    return X_train, y_train, X_test, y_test

def standardize_data(X, mean=None, std=None):
    if mean is None or std is None:
        mean = np.mean(X, axis=0)
        std = np.std(X, axis=0)
    std[std == 0] = 1
    X_std = (X - mean) / std
    return X_std, mean, std

def perform_pca(X, variance_threshold=0.8):
    mean = np.mean(X, axis=0)
    X_centered = X - mean
    cov_matrix = np.cov(X_centered, rowvar=False)
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
    sorted_idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sorted_idx]
    eigenvectors = eigenvectors[:, sorted_idx]
    explained_variances = eigenvalues / np.sum(eigenvalues)
    cumulative_variance = np.cumsum(explained_variances)
    k = np.searchsorted(cumulative_variance, variance_threshold) + 1
    eigenvectors_reduced = eigenvectors[:, :k]
    X_reduced = np.dot(X_centered, eigenvectors_reduced)
    return X_reduced, eigenvectors_reduced, mean, k, cumulative_variance

###############################################################################
# 2. SVM 與 RBF kernel
###############################################################################
def rbf_kernel(X1, X2, gamma):
    X1_sq = np.sum(X1**2, axis=1).reshape(-1, 1)
    X2_sq = np.sum(X2**2, axis=1).reshape(1, -1)
    dist_sq = X1_sq + X2_sq - 2 * np.dot(X1, X2.T)
    dist_sq = np.maximum(dist_sq, 0)
    return np.exp(-gamma * dist_sq)

def svm_train(X, y, C, kernel_func, gamma, tol=1e-4):
    n_samples = X.shape[0]
    K = kernel_func(X, X, gamma)
    P = matrix(np.outer(y, y) * K)
    q = matrix(-np.ones(n_samples))
    G_std = np.diag(-np.ones(n_samples))
    h_std = np.zeros(n_samples)
    G_slack = np.diag(np.ones(n_samples))
    h_slack = np.ones(n_samples) * C
    G = matrix(np.vstack((G_std, G_slack)))
    h = matrix(np.hstack((h_std, h_slack)))
    A = matrix(y.reshape(1, -1))
    b = matrix(np.zeros(1))
    solvers.options['show_progress'] = False
    solution = solvers.qp(P, q, G, h, A, b)
    alphas = np.ravel(solution['x'])
    sv = alphas > tol
    ind = np.arange(len(alphas))[sv]
    alphas_sv = alphas[sv]
    sv_X = X[sv]
    sv_y = y[sv]
    # 計算 b
    b_val = 0
    for n in range(len(alphas_sv)):
        b_val += sv_y[n] - np.sum(alphas_sv * sv_y * K[ind[n], sv])
    b_val /= len(alphas_sv)
    model = {
        'alphas': alphas_sv,
        'sv_X': sv_X,
        'sv_y': sv_y,
        'b': b_val,
        'gamma': gamma,
        'kernel_func': kernel_func
    }
    return model

def svm_decision_function(model, X):
    alphas = model['alphas']
    sv_X = model['sv_X']
    sv_y = model['sv_y']
    b_val = model['b']
    gamma = model['gamma']
    kernel_func = model['kernel_func']
    K = kernel_func(X, sv_X, gamma)
    decision_values = np.dot(K, alphas * sv_y) + b_val
    return decision_values

def svm_predict(model, X, use_calibration=False, threshold=0.5):
    decision_values = svm_decision_function(model, X)
    if use_calibration and 'calibration_method' in model:
        if model['calibration_method'] == 'temperature_scaling':
            T = model['calibration']
            prob = 1.0 / (1.0 + np.exp(-decision_values / T))
        else:
            # 預設: 不校準
            prob = 1.0 / (1.0 + np.exp(-decision_values))
        predictions = np.where(prob >= threshold, 1, -1)
        return predictions, prob
    else:
        predictions = np.sign(decision_values)
        return predictions, decision_values

###############################################################################
# 3. Temperature Scaling 校準 (LBFGS)
###############################################################################
def calibrate_model_temperature_scaling(model, X_calib, y_calib):
    decision_values = svm_decision_function(model, X_calib)
    y_calib_binary = np.where(y_calib == -1, 0, 1).astype(float)
    
    def objective(logT):
        T = np.exp(logT)
        x = decision_values / T
        p = 1.0 / (1.0 + np.exp(-x))
        eps = 1e-12
        ce = -np.sum(y_calib_binary * np.log(p + eps) + (1 - y_calib_binary) * np.log(1 - p + eps))
        return ce
    
    res = minimize(objective, x0=0.0, method='L-BFGS-B')
    best_logT = res.x[0]
    best_T = np.exp(best_logT)
    
    model['calibration_method'] = 'temperature_scaling'
    model['calibration'] = best_T
    return model

###############################################################################
# 4. 評估指標 (F1-score) 與閾值搜尋
###############################################################################
def compute_f1_score(y_true, y_pred):
    tp = np.sum((y_true == 1) & (y_pred == 1))
    fp = np.sum((y_true == -1) & (y_pred == 1))
    fn = np.sum((y_true == 1) & (y_pred == -1))
    if tp == 0:
        return 0.0
    precision = tp / (tp + fp) if (tp + fp) != 0 else 0.0
    recall = tp / (tp + fn) if (tp + fn) != 0 else 0.0
    if precision + recall == 0:
        return 0.0
    return 2 * precision * recall / (precision + recall)

def find_optimal_threshold_f1(probs, true_labels):
    thresholds = np.linspace(0, 1, 1001)
    best_thresh = 0.5
    best_f1 = 0.0
    for thresh in thresholds:
        preds = np.where(probs >= thresh, 1, -1)
        f1 = compute_f1_score(true_labels, preds)
        if f1 > best_f1:
            best_f1 = f1
            best_thresh = thresh
    return best_thresh, best_f1

def accuracy_score(y_true, y_pred):
    return np.mean(y_true == y_pred)

###############################################################################
# 5. 繪製校準曲線
###############################################################################
def plot_calibration_curve(probs, true_labels, n_bins=10):
    bin_edges = np.linspace(0, 1, n_bins + 1)
    bin_true = np.zeros(n_bins)
    bin_pred = np.zeros(n_bins)
    bin_counts = np.zeros(n_bins)
    
    for p, t in zip(probs, true_labels):
        bin_idx = np.digitize(p, bin_edges, right=True) - 1
        bin_idx = np.clip(bin_idx, 0, n_bins - 1)
        bin_true[bin_idx] += t
        bin_pred[bin_idx] += p
        bin_counts[bin_idx] += 1
        
    nonzero = bin_counts > 0
    avg_true = np.zeros(n_bins)
    avg_pred = np.zeros(n_bins)
    avg_true[nonzero] = bin_true[nonzero] / bin_counts[nonzero]
    avg_pred[nonzero] = bin_pred[nonzero] / bin_counts[nonzero]
    
    plt.figure(figsize=(8, 6))
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')
    plt.plot(avg_pred, avg_true, marker='o', linewidth=2, label='Calibration Curve')
    plt.xlabel('Mean Predicted Probability')
    plt.ylabel('Fraction of Positives')
    plt.title('Calibration Curve')
    plt.legend()
    plt.show()

###############################################################################
# 6. Random Search for (C, gamma)
###############################################################################
def random_search_f1(X, y, n_iter=30, c_range=(-2, 3), gamma_range=(-3, 1), 
                     validation_ratio=0.2, variance_threshold=0.8, tol=1e-4):
    """
    - n_iter: 進行多少次隨機抽樣
    - c_range: (low, high) 表示在 log10(C) 的區間
    - gamma_range: (low, high) 表示在 log10(gamma) 的區間
    """
    # 分割 hold-out
    n_samples = X.shape[0]
    indices = np.arange(n_samples)
    np.random.shuffle(indices)
    split = int(n_samples * (1 - validation_ratio))
    train_idx = indices[:split]
    val_idx = indices[split:]
    
    X_train_hold = X[train_idx]
    y_train_hold = y[train_idx]
    X_val_hold = X[val_idx]
    y_val_hold = y[val_idx]
    
    # 標準化與 PCA
    X_train_hold_std, hold_mean, hold_std = standardize_data(X_train_hold)
    X_val_hold_std, _, _ = standardize_data(X_val_hold, mean=hold_mean, std=hold_std)
    
    X_train_hold_pca, hold_pca_components, hold_pca_mean, k, _ = perform_pca(X_train_hold_std, variance_threshold=variance_threshold)
    X_val_hold_centered = X_val_hold_std - hold_pca_mean
    X_val_hold_pca = np.dot(X_val_hold_centered, hold_pca_components)
    
    best_combo = None
    best_f1 = -1
    
    results = {}
    
    for i in range(n_iter):
        # loguniform: 先對 log10(C) 做均勻抽樣
        logC = random.uniform(c_range[0], c_range[1])  # e.g. c_range=(-2, 3) => C in [10^-2, 10^3]
        C = 10**logC
        
        logGamma = random.uniform(gamma_range[0], gamma_range[1])  # e.g. gamma in [10^-3, 10^1]
        gamma = 10**logGamma
        
        model = svm_train(X_train_hold_pca, y_train_hold, C, rbf_kernel, gamma, tol)
        y_val_pred, _ = svm_predict(model, X_val_hold_pca)
        f1 = compute_f1_score(y_val_hold, y_val_pred)
        results[(C, gamma)] = f1
        
        if f1 > best_f1:
            best_f1 = f1
            best_combo = (C, gamma)
    
    return best_combo, results

###############################################################################
# 7. 主流程
###############################################################################
def main():
    # 選擇 'random_search' 作為搜尋方法
    # (若不想校準，可將 calibration_method = 'none')
    calibration_method = 'temperature_scaling'
    
    train_file = 'data/train.csv'
    test_file = 'data/test.csv'
    
    print("讀取訓練與測試資料...")
    X_train, y_train, X_test, y_test = load_data(train_file, test_file)
    
    # 轉換標籤 (0 -> -1)
    y_train = np.where(y_train == 0, -1, 1).astype(np.float64)
    y_test = np.where(y_test == 0, -1, 1).astype(np.float64)
    
    print("對訓練資料進行標準化...")
    X_train_std, train_mean, train_std = standardize_data(X_train)
    X_test_std, _, _ = standardize_data(X_test, mean=train_mean, std=train_std)
    
    print("對訓練資料執行 PCA ...")
    X_train_pca, pca_components, pca_mean, k, cumulative_variance = perform_pca(X_train_std, variance_threshold=0.8)
    print(f"PCA 完成，保留 {k} 個主成分")
    
    X_test_centered = X_test_std - pca_mean
    X_test_pca = np.dot(X_test_centered, pca_components)
    
    # -------------------------------
    # 使用 Random Search (C, gamma)
    # -------------------------------
    print("\n開始 Random Search (F1-score) ...")
    best_combo, search_results = random_search_f1(X_train, y_train, n_iter=30,
                                                  c_range=(-2, 3), gamma_range=(-3, 1),
                                                  validation_ratio=0.2, variance_threshold=0.8)
    best_c, best_gamma = best_combo
    print("各組合 (C, gamma) -> F1-score:")
    for combo, f1_val in search_results.items():
        print(f"  C={combo[0]:.4f}, gamma={combo[1]:.4f} -> F1-score: {f1_val*100:.2f}%")
    
    print(f"\nRandom Search 選出的最佳 (C, gamma) = ({best_c}, {best_gamma})")
    
    # -------------------------------
    # 以最佳參數重新訓練完整模型
    # -------------------------------
    print(f"\n使用最佳 (C={best_c}, gamma={best_gamma}) 重新訓練模型 ...")
    best_model = svm_train(X_train_pca, y_train, best_c, rbf_kernel, best_gamma)
    
    y_train_pred, _ = svm_predict(best_model, X_train_pca)
    train_acc = accuracy_score(y_train, y_train_pred)
    y_test_pred, _ = svm_predict(best_model, X_test_pca)
    test_acc = accuracy_score(y_test, y_test_pred)
    print(f"完整訓練集 Accuracy: {train_acc*100:.2f}%，測試集 Accuracy: {test_acc*100:.2f}%")
    
    # -------------------------------
    # 溫度縮放校準 (LBFGS) 或不校準
    # -------------------------------
    if calibration_method == 'temperature_scaling':
        print("使用 Temperature Scaling (LBFGS) 校準...")
        best_model = calibrate_model_temperature_scaling(best_model, X_test_pca, y_test)
        print("校準完成。")
    else:
        print("不使用校準...")
    
    # 在測試集上搜尋最佳閾值（以 F1-score 為依據）
    _, test_probs = svm_predict(best_model, X_test_pca, use_calibration=(calibration_method != 'none'))
    optimal_thresh, best_f1 = find_optimal_threshold_f1(test_probs, y_test)
    print(f"從測試集選出的最佳閾值為 {optimal_thresh:.3f} (F1-score: {best_f1*100:.2f}%)")
    
    # 繪製校準曲線
    y_test_binary = np.where(y_test == -1, 0, 1)
    plot_calibration_curve(test_probs, y_test_binary, n_bins=10)
    
    # -------------------------------
    # 讀取 evaluation 資料並進行預測
    # -------------------------------
    eval_df = pd.read_csv("data/eval.anon.csv")
    eval_ids = pd.read_csv("data/eval.id", header=None, names=["id"])
    
    X_eval = eval_df.filter(regex="^x").values
    X_eval_std, _, _ = standardize_data(X_eval, mean=train_mean, std=train_std)
    X_eval_centered = X_eval_std - pca_mean
    X_eval_pca = np.dot(X_eval_centered, pca_components)
    
    eval_pred, eval_prob = svm_predict(best_model, X_eval_pca, use_calibration=(calibration_method != 'none'),
                                       threshold=optimal_thresh)
    eval_pred_converted = np.where(eval_pred == -1, 0, 1)
    
    output_df = pd.DataFrame({"example_id": eval_ids["id"], "label": eval_pred_converted})
    output_folder = "SVM"
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    output_path = os.path.join(output_folder, "svm_eval_predictions_random.csv")
    output_df.to_csv(output_path, index=False)
    print(f"Evaluation 預測結果已寫入：{output_path}")

if __name__ == "__main__":
    main()
