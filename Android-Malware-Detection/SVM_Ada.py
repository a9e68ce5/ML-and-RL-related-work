import os
import numpy as np
import pandas as pd
import pymrmr
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, classification_report, precision_recall_curve
from SVM_handcraft import SVC  # 請確認此模組中有 SVC，且已設定 probability=True
from DecisionTree_weightedGini import DecisionTree  # 請確認此模組中的類別名稱為 DecisionTree

#--------------------------------------------------
# 弱分類器：DecayingLearningRatePerceptron (DecayPerceptron)
#--------------------------------------------------
class DecayingLearningRatePerceptron:
    def __init__(self, n_features, learning_rate=0.1, n_iter=3000, class_weights=None):
        self.n_features = n_features
        self.initial_lr = learning_rate    # 初始學習率
        self.n_iter = n_iter
        self.class_weights = class_weights
        self.weights = np.random.uniform(-0.01, 0.01, size=n_features)
        self.bias = np.random.uniform(-0.01, 0.01)
        self.t = 0  # 全局時間步

    def fit(self, X, y):
        # 如果有 class_weights 則建立樣本權重，否則皆為1
        if self.class_weights is not None:
            sample_weights = np.array([self.class_weights[label] for label in y])
        else:
            sample_weights = np.ones(len(y), dtype=float)
        # 將標籤轉為 {-1,1}（假設原始標籤為0與1）
        unique = np.unique(y)
        y_conv = np.where(y == 0, -1, 1) if set(unique) == {0, 1} else y

        for epoch in range(self.n_iter):
            errors = 0
            current_lr = self.initial_lr / (1 + self.t)
            for xi, target, s_weight in zip(X, y_conv, sample_weights):
                if target * (np.dot(xi, self.weights) + self.bias) <= 0:
                    self.weights += current_lr * s_weight * target * xi
                    self.bias += current_lr * s_weight * target
                    errors += 1
            self.t += 0.001
            if errors == 0:
                break

    def predict(self, X):
        net = np.dot(X, self.weights) + self.bias
        preds = np.where(net >= 0, 1, -1)
        # 轉換 {-1,1}為 {0,1}（若需要後續調整可再轉換）
        return np.where(preds == -1, 0, preds)

#--------------------------------------------------
# 輔助函數：資料讀取、標準化與特徵選取
#--------------------------------------------------
def load_and_preprocess(file_path, scaler=None, selected_features=None, fit_scaler=False):
    df = pd.read_csv(file_path)
    feature_cols = [col for col in df.columns if col.startswith('x')]
    X = df[feature_cols]
    y = df['label']
    
    if scaler is None and fit_scaler:
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(X)
    elif scaler is not None:
        X_scaled = scaler.transform(X)
    else:
        X_scaled = X.values
    X_scaled = pd.DataFrame(X_scaled, columns=feature_cols)
    
    if selected_features is None and fit_scaler:
        # pymrmr 要求第一欄必須為 label
        df_mrmr = pd.concat([y.reset_index(drop=True), X_scaled.reset_index(drop=True)], axis=1)
        df_mrmr.columns = ['label'] + feature_cols
        n_features_to_select = 200  # 可根據需求調整
        selected_features = pymrmr.mRMR(df_mrmr, 'MIQ', n_features_to_select)
        print("Selected features:", selected_features)
    
    if selected_features is not None:
        X_final = X_scaled[selected_features]
    else:
        X_final = X_scaled
        
    return X_final, y, scaler, selected_features

def convert_labels(y):
    unique = np.unique(y)
    if set(unique) != {-1, 1}:
        return np.where(y == 0, -1, 1)
    return y

def adjust_pred(pred):
    # 將模型 predict 得到的 0 轉為 -1 供 AdaBoost 更新使用
    return np.where(pred == 0, -1, pred)

def map_minus1_to_0(arr):
    return np.where(arr == -1, 0, arr)

#--------------------------------------------------
# 主流程：建立 AdaBoost 模型
#--------------------------------------------------
# 讀取資料
X_train, y_train, scaler, selected_features = load_and_preprocess('data/train.csv', fit_scaler=True)
X_test, y_test, _, _ = load_and_preprocess('data/test.csv', scaler=scaler, selected_features=selected_features)
X_eval, y_eval, _, _ = load_and_preprocess('data/eval.anon.csv', scaler=scaler, selected_features=selected_features)

y_train = convert_labels(y_train)
y_test  = convert_labels(y_test)
y_eval  = convert_labels(y_eval)

# AdaBoost 參數設定
T = 100  # boosting 輪數
N = X_train.shape[0]
weights = np.ones(N) / N

weak_learners = []  # 儲存每輪弱分類器
alphas = []         # 儲存各輪弱分類器權重
train_weak_preds = np.zeros((N, T))  # 訓練集上各輪弱分類器的預測

for t in range(T):
    n_feat = X_train.values.shape[1]
    
    # 弱分類器1：使用 DecayingLearningRatePerceptron (Accuracy 約 0.72)
    decay_perceptron = DecayingLearningRatePerceptron(n_features=n_feat, learning_rate=1.0, n_iter=1000, class_weights=None)
    decay_perceptron.fit(X_train.values, y_train)
    pred_decay = decay_perceptron.predict(X_train.values)
    pred_decay = adjust_pred(pred_decay)
    err_decay = np.sum(weights * (pred_decay != y_train)) / np.sum(weights)
    
    # 弱分類器2：使用 DecisionTree (Accuracy 約 0.8)
    tree = DecisionTree(max_depth=10, min_samples_split=2)
    tree.fit(X_train.values, y_train, sample_weights=weights)
    pred_tree = tree.predict(X_train.values)
    pred_tree = adjust_pred(pred_tree)
    err_tree = np.sum(weights * (pred_tree != y_train)) / np.sum(weights)
    
    # 選擇錯誤率較低的分類器作為本輪弱分類器
    if err_decay <= err_tree:
        chosen_model = decay_perceptron
        chosen_pred = pred_decay
        err = err_decay
    else:
        chosen_model = tree
        chosen_pred = pred_tree
        err = err_tree
    
    # 避免極小或極大錯誤率
    eps = 1e-10
    err = np.clip(err, eps, 1 - eps)
    alpha = 0.5 * np.log((1 - err) / err)
    alphas.append(alpha)
    weak_learners.append(chosen_model)
    train_weak_preds[:, t] = chosen_pred
    
    # 更新樣本權重 (公式: w_i <- w_i * exp(-alpha * y_i * h(x_i)))
    weights = weights * np.exp(-alpha * y_train * chosen_pred)
    weights = weights / np.sum(weights)
    
    print(f"Iteration {t+1}/{T}: Selected {type(chosen_model).__name__}, error = {err:.4f}, alpha = {alpha:.4f}")

#--------------------------------------------------
# 堆疊 SVM 強分類器 (單獨 SVM 準確率約 0.81)
#--------------------------------------------------
svm_features_train = train_weak_preds
svm_model = SVC(probability=True)
svm_model.fit(svm_features_train, y_train)
print("SVC (強分類器) 訓練完成。")

# 訓練集評估
train_preds = svm_model.predict(svm_features_train)
train_preds_mapped = map_minus1_to_0(train_preds)
y_train_mapped = map_minus1_to_0(y_train)
train_accuracy = accuracy_score(y_train_mapped, train_preds_mapped)
print("Train Accuracy:", train_accuracy)
print("Train Classification Report:\n", classification_report(y_train_mapped, train_preds_mapped))

def get_weak_preds(learners, X, T):
    N_samples = X.shape[0]
    preds = np.zeros((N_samples, T))
    for i, learner in enumerate(learners):
        pred = learner.predict(X.values)
        preds[:, i] = adjust_pred(pred)
    return preds

svm_features_test = get_weak_preds(weak_learners, X_test, T)
pred_test = svm_model.predict(svm_features_test)
pred_test_mapped = map_minus1_to_0(pred_test)
y_test_mapped = map_minus1_to_0(y_test)
test_accuracy = accuracy_score(y_test_mapped, pred_test_mapped)
test_f1 = f1_score(y_test_mapped, pred_test_mapped, average='weighted')
print("Test Accuracy:", test_accuracy)
print("Test F1 Score:", test_f1)
print("Test Classification Report:\n", classification_report(y_test_mapped, pred_test_mapped))

#--------------------------------------------------
# 自動調整決策門檻
#--------------------------------------------------
y_proba_test = svm_model.predict_proba(svm_features_test)[:, 1]
precision, recall, thresholds = precision_recall_curve(y_test_mapped, y_proba_test)
f1_scores = 2 * precision * recall / (precision + recall + 1e-8)
best_idx = np.argmax(f1_scores)
best_threshold = thresholds[best_idx] if len(thresholds) > 0 else 0.5
print("Best threshold based on F1:", best_threshold)
pred_test_adj = (y_proba_test >= best_threshold).astype(int)
print("Test Classification Report (adjusted threshold):\n", classification_report(y_test_mapped, pred_test_adj))

#--------------------------------------------------
# 評估資料預測與輸出
#--------------------------------------------------
svm_features_eval = get_weak_preds(weak_learners, X_eval, T)
y_proba_eval = svm_model.predict_proba(svm_features_eval)[:, 1]
pred_eval_adj = (y_proba_eval >= best_threshold).astype(int)
eval_ids = pd.read_csv('data/eval.id', header=None, names=['id'])
output_df = pd.DataFrame({"example_id": eval_ids['id'], "label": pred_eval_adj})
output_folder = "AdaBoost"
if not os.path.exists(output_folder):
    os.makedirs(output_folder)
output_path = os.path.join(output_folder, "adaboost_eval_predictions.csv")
output_df.to_csv(output_path, index=False)
print("Evaluation predictions saved to:", output_path)
