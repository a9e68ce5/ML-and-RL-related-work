import os
import pandas as pd
import numpy as np

#######################################
# 隨機傅立葉特徵 (Random Fourier Features)
#######################################
def rff_transform(X, D, gamma):
    """
    對資料 X 進行隨機傅立葉特徵映射
    輸入:
      X: 原始資料，形狀 (n_samples, n_features)
      D: 隨機特徵數量
      gamma: RBF 核參數
    輸出:
      Z: 映射後的特徵，形狀 (n_samples, D)
      W: 隨機投影矩陣，形狀 (n_features, D)
      b: 隨機偏置向量，形狀 (D,)
    """
    n, d = X.shape
    W = np.random.normal(0, np.sqrt(2 * gamma), size=(d, D))
    b = np.random.uniform(0, 2 * np.pi, size=D)
    Z = np.sqrt(2.0 / D) * np.cos(np.dot(X, W) + b)
    return Z, W, b

#######################################
# hinge_loss_and_grad (次梯度下降法)
#######################################
def hinge_loss_and_grad(X, y, w, b, C):
    """
    計算 hinge loss 與對 w 與 b 的次梯度
    loss = 0.5 * ||w||^2 + C * sum_i max(0, 1 - y_i*(w·x_i + b))
    次梯度：
      grad_w = w - C * sum_{i: margin > 0} y_i * x_i
      grad_b = - C * sum_{i: margin > 0} y_i
    """
    predictions = X.dot(w) + b
    margins = 1 - y * predictions
    hinge_losses = np.maximum(0, margins)
    loss = 0.5 * np.sum(w ** 2) + C * np.sum(hinge_losses)
    
    grad_w = w.copy()
    grad_b = 0.0
    for i in range(len(y)):
        if margins[i] > 0:
            grad_w -= C * y[i] * X[i]
            grad_b -= C * y[i]
    return loss, grad_w, grad_b

#######################################
# 預測與準確率計算函數
#######################################
def compute_accuracy(X, y, w, b):
    preds = np.where(X.dot(w) + b > 0, 1, -1)
    return np.mean(preds == y)

#######################################
# SVM 訓練函數 (次梯度下降法)
#######################################
def train_svm_subgradient(X_train, y_train, C, learning_rate=1e-3, epochs=100):
    """
    使用次梯度下降法訓練 SVM 模型（在 RFF 空間下訓練，相當於非線性 SVM）
    每個 epoch 更新參數，並印出 loss 與訓練準確率
    """
    d = X_train.shape[1]
    w = np.zeros(d)
    b = 0.0
    best_train_acc = 0.0
    best_w, best_b = w.copy(), b

    for epoch in range(epochs):
        loss, grad_w, grad_b = hinge_loss_and_grad(X_train, y_train, w, b, C)
        w -= learning_rate * grad_w
        b -= learning_rate * grad_b

        train_acc = compute_accuracy(X_train, y_train, w, b)
        print(f"Epoch {epoch+1}: Loss = {loss:.4f}, Train Acc = {train_acc:.4f}")
        if train_acc > best_train_acc:
            best_train_acc = train_acc
            best_w, best_b = w.copy(), b
            print("  Best model updated.")
    return best_w, best_b

#######################################
# k-fold 分割函數
#######################################
def k_fold_split(X, y, k=5):
    n_samples = X.shape[0]
    indices = np.arange(n_samples)
    np.random.shuffle(indices)
    folds = np.array_split(indices, k)
    return folds

#######################################
# 交叉驗證：使用 RFF 與次梯度 SVM 進行 k-fold 評估，並回傳平均準確率
#######################################
def cross_validate_svm_subgradient(X, y, k, C, learning_rate, epochs, D, gamma):
    folds = k_fold_split(X, y, k)
    accs = []
    for i in range(k):
        # 分割訓練與驗證 fold
        val_indices = folds[i]
        train_indices = np.concatenate([folds[j] for j in range(k) if j != i])
        X_train_cv = X[train_indices]
        y_train_cv = y[train_indices]
        X_val_cv = X[val_indices]
        y_val_cv = y[val_indices]
        
        # 標準化：以訓練 fold 為基準
        X_train_cv, mean_cv, std_cv = standardize(X_train_cv)
        X_val_cv, _, _ = standardize(X_val_cv, mean_cv, std_cv)
        
        # RFF 映射：在訓練 fold 上計算隨機映射，再用相同映射參數處理驗證 fold
        Z_train_cv, W_rff, b_rff = rff_transform(X_train_cv, D, gamma)
        Z_val_cv = np.sqrt(2.0 / D) * np.cos(np.dot(X_val_cv, W_rff) + b_rff)
        
        # 在 RFF 空間下訓練 SVM 模型
        best_w, best_b = train_svm_subgradient(Z_train_cv, y_train_cv, C, learning_rate, epochs)
        val_acc = compute_accuracy(Z_val_cv, y_val_cv, best_w, best_b)
        print(f"Fold {i+1}: Validation Accuracy = {val_acc:.4f}")
        accs.append(val_acc)
    avg_acc = np.mean(accs)
    print(f"Gamma = {gamma}, Average CV Accuracy = {avg_acc:.4f}\n")
    return avg_acc

#######################################
# 資料準備函數
#######################################
def prepare_data(df, feature_prefix="x"):
    feature_cols = [col for col in df.columns if col.startswith(feature_prefix)]
    X = df[feature_cols].values.astype(float)
    y = df["label"].values
    # 將 0 轉換為 -1（SVM 使用 -1 與 1）
    y = np.where(y == 0, -1, 1)
    return X, y

#######################################
# 資料標準化 (使用 numpy) 同上
#######################################
def standardize(X, mean=None, std=None):
    if mean is None:
        mean = np.mean(X, axis=0)
    if std is None:
        std = np.std(X, axis=0)
        std[std == 0] = 1.0
    return (X - mean) / std, mean, std

#######################################
# 主程式
#######################################
def main():
    # 讀取資料
    train_df = pd.read_csv("data/train.csv")
    test_df  = pd.read_csv("data/test.csv")
    eval_df  = pd.read_csv("data/eval.anon.csv")
    eval_ids = pd.read_csv("data/eval.id", header=None, names=["id"])

    # 準備資料：特徵以 "x" 開頭，label 欄位
    X_train, y_train = prepare_data(train_df)
    X_test, y_test   = prepare_data(test_df)
    X_eval, y_eval   = prepare_data(eval_df)

    # 資料標準化 (以訓練資料為基準)
    X_train, mean_train, std_train = standardize(X_train)
    X_test, _, _ = standardize(X_test, mean_train, std_train)
    X_eval, _, _ = standardize(X_eval, mean_train, std_train)

    ###################################################
    # RFF 與交叉驗證：調整 RBF 核參數 gamma
    ###################################################
    D = 500             # 隨機特徵數量
    C = 1.0
    learning_rate = 1e-3
    epochs = 6000
    k = 5
    # 定義候選 gamma 值
    candidate_gammas = [0.001, 0.01, 0.05, 0.1, 1.0]
    best_gamma = None
    best_cv_acc = -np.inf
    for gamma in candidate_gammas:
        print(f"Testing gamma = {gamma}")
        cv_acc = cross_validate_svm_subgradient(X_train, y_train, k, C, learning_rate, epochs, D, gamma)
        if cv_acc > best_cv_acc:
            best_cv_acc = cv_acc
            best_gamma = gamma
    print(f"最佳 gamma 值: {best_gamma}, Cross-Validation Average Accuracy = {best_cv_acc:.4f}\n")

    ###################################################
    # 最終模型訓練：使用全體訓練資料 (RFF 空間)
    ###################################################
    Z_train, W_rff, b_rff = rff_transform(X_train, D, best_gamma)
    Z_test  = np.sqrt(2.0 / D) * np.cos(np.dot(X_test, W_rff) + b_rff)
    Z_eval  = np.sqrt(2.0 / D) * np.cos(np.dot(X_eval, W_rff) + b_rff)

    print("開始在全體訓練資料 (RFF 空間) 下訓練最終模型")
    best_w, best_b = train_svm_subgradient(Z_train, y_train, C, learning_rate, epochs)
    train_acc = compute_accuracy(Z_train, y_train, best_w, best_b)
    test_acc  = compute_accuracy(Z_test, y_test, best_w, best_b)
    print(f"Final Train Accuracy: {train_acc:.4f}")
    print(f"Test Accuracy: {test_acc:.4f}")

    # 預測 eval 資料 (將 -1 轉換為 0)
    preds_eval = np.where(Z_eval.dot(best_w) + best_b > 0, 1, 0)
    eval_results = pd.DataFrame({
        "example_id": eval_ids["id"],
        "label": preds_eval
    })
    output_folder = "SVM"
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    output_filename = os.path.join(output_folder, "svm_eval_predictions_rff_cv.csv")
    eval_results.to_csv(output_filename, index=False)
    print(f"Evaluation 結果已存檔至 {output_filename}")

if __name__ == "__main__":
    main()
