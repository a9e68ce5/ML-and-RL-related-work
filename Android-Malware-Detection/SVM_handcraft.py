import os
import numpy as np
import pandas as pd
import pymrmr
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.metrics import f1_score, accuracy_score, classification_report, precision_recall_curve
from sklearn.model_selection import cross_val_score, StratifiedKFold
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import optuna

# -----------------------------
# 1. 載入並標準化訓練資料
# -----------------------------
train_df = pd.read_csv('data/train.csv')
# 假設特徵欄位皆以 "x" 開頭
feature_cols = [col for col in train_df.columns if col.startswith('x')]
X_train = train_df[feature_cols]
y_train = train_df['label']

# 使用 StandardScaler 進行標準化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols)

# -----------------------------
# 2. 使用 pymrmr 進行特徵選取
# -----------------------------
# pymrmr 要求第一欄必須為 label，所以先合併後重新排列欄位
df_for_mrmr = pd.concat([y_train.reset_index(drop=True), X_train_scaled.reset_index(drop=True)], axis=1)
df_for_mrmr.columns = ['label'] + feature_cols
# 這裡以選取 100 個特徵為例（可根據需要調整）110:0.8751 250
n_features = 150 
selected_features = pymrmr.mRMR(df_for_mrmr, 'MIQ', n_features)
print("Selected features:", selected_features)

X_train_selected = X_train_scaled[selected_features]

# -----------------------------
# 3. 定義 Optuna 優化目標函數
# -----------------------------
def objective(trial):
    # 目前少數類別比例約 0.51，故將下界設為 0.55 避免 SMOTE 嘗試移除樣本
    sampling_strategy = trial.suggest_float("smote__sampling_strategy", 0.55, 1.0)
    k_neighbors = trial.suggest_int("smote__k_neighbors", 3, 7)
    
    C = trial.suggest_float("svm__C", 0.1, 10, log=True)
    gamma = trial.suggest_float("svm__gamma", 0.001, 1, log=True)
    
    pipeline = ImbPipeline(steps=[
        ('smote', SMOTE(sampling_strategy=sampling_strategy, 
                        k_neighbors=k_neighbors, 
                        random_state=42)),
        ('svm', SVC(C=C, gamma=gamma, probability=True))
    ])
    
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
    scores = cross_val_score(pipeline, X_train_selected, y_train, cv=cv, scoring='f1_weighted', n_jobs=-1)
    return np.mean(scores)

# -----------------------------
# 4. 使用 Optuna 進行超參數搜尋
# -----------------------------
study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=100)  # 試驗次數可依需求增加

print("Best trial:")
best_trial = study.best_trial
print("  Best weighted F1 score: {:.4f}".format(best_trial.value))
print("  Best hyperparameters:")
for key, value in best_trial.params.items():
    print("    {}: {}".format(key, value))

# -----------------------------
# 5. 使用最佳參數訓練模型並評估
# -----------------------------
best_pipeline = ImbPipeline(steps=[
    ('smote', SMOTE(sampling_strategy=best_trial.params["smote__sampling_strategy"],
                    k_neighbors=best_trial.params["smote__k_neighbors"],
                    random_state=42)),
    ('svm', SVC(C=best_trial.params["svm__C"],
                gamma=best_trial.params["svm__gamma"],
                probability=True))
])
best_pipeline.fit(X_train_selected, y_train)

y_train_pred = best_pipeline.predict(X_train_selected)
train_accuracy = accuracy_score(y_train, y_train_pred)
print("Training Accuracy after tuning:", train_accuracy)
print("Training Classification Report:\n", classification_report(y_train, y_train_pred))


# -----------------------------
# 6. 載入測試資料並評估模型表現
# -----------------------------
test_df = pd.read_csv('data/test.csv')
X_test = test_df[feature_cols]
y_test = test_df['label']

X_test_scaled = scaler.transform(X_test)
X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols)
X_test_selected = X_test_scaled[selected_features]

y_test_pred = best_pipeline.predict(X_test_selected)
test_accuracy = accuracy_score(y_test, y_test_pred)
test_f1 = f1_score(y_test, y_test_pred, average='weighted')
print("Test Accuracy after tuning:", test_accuracy)
print("Test F1 Score after tuning:", test_f1)
print("Test Classification Report:\n", classification_report(y_test, y_test_pred))

# -----------------------------
# 7. 自動尋找最佳決策門檻
# -----------------------------
# 取得測試資料的正類預測機率
y_proba_test = best_pipeline.predict_proba(X_test_selected)[:, 1]
precision, recall, thresh_arr = precision_recall_curve(y_test, y_proba_test)
# 計算各門檻下的 F1 score
f1_scores = 2 * precision * recall / (precision + recall + 1e-8)
# 由 precision_recall_curve 會回傳 len(thresh_arr) = len(precision)-1，所以取最大 F1 score的索引加一
best_index = np.argmax(f1_scores)
# 若 best_index 為 0 代表 precision_recall_curve 未產生有效門檻，此時可直接使用預設 0.5
best_threshold = thresh_arr[best_index] if len(thresh_arr) > 0 else 0.6
print("Best threshold based on F1 score: {:.3f}".format(best_threshold))

# 使用最佳門檻重新計算測試集預測結果
y_test_pred_adj = (y_proba_test >= best_threshold).astype(int)
print("Test Classification Report (adjusted threshold):")
print(classification_report(y_test, y_test_pred_adj))
# -----------------------------
# 7. 載入評估資料並產生預測結果
# -----------------------------
eval_df = pd.read_csv('data/eval.anon.csv')
eval_ids = pd.read_csv('data/eval.id', header=None, names=['id'])

X_eval = eval_df[feature_cols]
X_eval_scaled = scaler.transform(X_eval)
X_eval_scaled = pd.DataFrame(X_eval_scaled, columns=feature_cols)
X_eval_selected = X_eval_scaled[selected_features]

# 取得評估資料的預測機率，並依據最佳門檻產生分類結果
y_eval_proba = best_pipeline.predict_proba(X_eval_selected)[:, 1]
y_eval_pred = (y_eval_proba >= best_threshold).astype(int)

output_df = pd.DataFrame({"example_id": eval_ids["id"], "label": y_eval_pred})
output_folder = "SVM"
if not os.path.exists(output_folder):
    os.makedirs(output_folder)
output_path = os.path.join(output_folder, "svm_eval_predictions_mRMR 250.csv")
output_df.to_csv(output_path, index=False)
print(f"Evaluation 預測結果已寫入：{output_path}")
