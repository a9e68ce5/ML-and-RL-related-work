import numpy as np
import pandas as pd
import os
import matplotlib.pyplot as plt
from cvxopt import matrix, solvers
from skopt import gp_minimize
from skopt.space import Real
from sklearn.svm import SVC
from sklearn.metrics import f1_score, accuracy_score
from sklearn.model_selection import train_test_split
from scipy.optimize import minimize
import torch
import torch.nn as nn
import torch.optim as optim

###############################################
# 1. 資料讀取、標準化、PCA
###############################################
def load_data(train_file, test_file):
    train_df = pd.read_csv(train_file)
    test_df = pd.read_csv(test_file)
    X_train = train_df.filter(regex="^x").values
    y_train = train_df['label'].values
    X_test = test_df.filter(regex="^x").values
    y_test = test_df['label'].values
    return X_train, y_train, X_test, y_test

def standardize_data(X, mean=None, std=None):
    if mean is None or std is None:
        mean = np.mean(X, axis=0)
        std = np.std(X, axis=0)
    std[std == 0] = 1
    X_std = (X - mean) / std
    return X_std, mean, std

def perform_pca(X, variance_threshold=0.8):
    mean = np.mean(X, axis=0)
    X_centered = X - mean
    cov_matrix = np.cov(X_centered, rowvar=False)
    eigenvalues, eigenvectors = np.linalg.eigh(cov_matrix)
    sorted_idx = np.argsort(eigenvalues)[::-1]
    eigenvalues = eigenvalues[sorted_idx]
    eigenvectors = eigenvectors[:, sorted_idx]
    explained_variances = eigenvalues / np.sum(eigenvalues)
    cumulative_variance = np.cumsum(explained_variances)
    k = np.searchsorted(cumulative_variance, variance_threshold) + 1
    eigenvectors_reduced = eigenvectors[:, :k]
    X_reduced = np.dot(X_centered, eigenvectors_reduced)
    return X_reduced, eigenvectors_reduced, mean, k, cumulative_variance

###############################################
# 2. SVM 與 RBF kernel (使用 scikit-learn SVC)
###############################################
def train_svm(X, y, C, gamma):
    clf = SVC(C=C, gamma=gamma, kernel='rbf', probability=False)
    clf.fit(X, y)
    return clf

###############################################
# 3. 評估指標
###############################################
def compute_f1_score(y_true, y_pred):
    return f1_score(y_true, y_pred, pos_label=1)

###############################################
# 4. Bayesian Optimization 搜尋 (C, gamma)
###############################################
def bayes_opt_f1(X, y, n_calls=30, c_bounds=(-2, 3), gamma_bounds=(-3, 1),
                 validation_ratio=0.2, variance_threshold=0.8, random_state=42):
    # 將資料分割成訓練集與驗證集
    np.random.seed(random_state)
    n_samples = X.shape[0]
    indices = np.arange(n_samples)
    np.random.shuffle(indices)
    split = int(n_samples * (1 - validation_ratio))
    train_idx = indices[:split]
    val_idx = indices[split:]
    X_train = X[train_idx]
    y_train = y[train_idx]
    X_val = X[val_idx]
    y_val = y[val_idx]
    
    # 標準化與 PCA (以訓練集為基準)
    X_train_std, mean_train, std_train = standardize_data(X_train)
    X_val_std, _, _ = standardize_data(X_val, mean=mean_train, std=std_train)
    X_train_pca, pca_components, pca_mean, k, _ = perform_pca(X_train_std, variance_threshold)
    X_val_centered = X_val_std - pca_mean
    X_val_pca = np.dot(X_val_centered, pca_components)
    
    # 定義目標函式：返回負 F1-score (因 gp_minimize 會最小化目標函式)
    def objective(params):
        logC, logGamma = params
        C = 10**logC
        gamma = 10**logGamma
        clf = train_svm(X_train_pca, y_train, C, gamma)
        y_val_pred = clf.predict(X_val_pca)
        f1 = compute_f1_score(y_val, y_val_pred)
        return -f1
    
    # 定義搜尋空間
    space  = [Real(c_bounds[0], c_bounds[1], name='logC'),
              Real(gamma_bounds[0], gamma_bounds[1], name='logGamma')]
    
    result = gp_minimize(objective, space, n_calls=n_calls, random_state=random_state)
    best_logC, best_logGamma = result.x
    best_C = 10**best_logC
    best_gamma = 10**best_logGamma
    
    # 儲存所有評估結果 (以方便後續檢查)
    results = {}
    for params, fval in zip(result.x_iters, result.func_vals):
        C_val = 10**params[0]
        gamma_val = 10**params[1]
        results[(C_val, gamma_val)] = -fval  # 轉回 F1-score
    
    return (best_C, best_gamma), results

###############################################
# 5. Temperature Scaling 校準 (PyTorch + LBFGS)
###############################################
def calibrate_temperature(clf, X_val, y_val):
    # 使用 SVC 的 decision_function 作為 logits
    logits = clf.decision_function(X_val)
    y_val_binary = np.where(y_val == -1, 0, 1).astype(float)
    
    # 轉換成 torch tensor
    logits_tensor = torch.tensor(logits, dtype=torch.float32)
    y_tensor = torch.tensor(y_val_binary, dtype=torch.float32)
    
    # 使用 logT 保證 T>0
    logT = torch.tensor(0.0, requires_grad=True)
    optimizer = optim.LBFGS([logT], max_iter=100, line_search_fn='strong_wolfe')
    
    def closure():
        optimizer.zero_grad()
        T = torch.exp(logT)
        scaled_logits = logits_tensor / T
        loss = nn.functional.binary_cross_entropy_with_logits(scaled_logits, y_tensor)
        loss.backward()
        return loss
    
    optimizer.step(closure)
    best_T = torch.exp(logT).item()
    return best_T

def temperature_scale_predictions(clf, X, T):
    logits = clf.decision_function(X)
    scaled_logits = logits / T
    prob = 1.0 / (1.0 + np.exp(-scaled_logits))
    return prob

###############################################
# 6. 細粒度閾值搜尋 (以 F1-score 為指標)
###############################################
def find_optimal_threshold_f1(probs, true_labels):
    thresholds = np.linspace(0, 1, 1001)
    best_thresh = 0.5
    best_f1 = 0.0
    for thresh in thresholds:
        preds = np.where(probs >= thresh, 1, -1)
        f1 = f1_score(true_labels, preds, pos_label=1)
        if f1 > best_f1:
            best_f1 = f1
            best_thresh = thresh
    return best_thresh, best_f1

###############################################
# 7. 繪製校準曲線
###############################################
def plot_calibration_curve(probs, true_labels, n_bins=10):
    bin_edges = np.linspace(0, 1, n_bins + 1)
    bin_true = np.zeros(n_bins)
    bin_pred = np.zeros(n_bins)
    bin_counts = np.zeros(n_bins)
    for p, t in zip(probs, true_labels):
        bin_idx = np.digitize(p, bin_edges, right=True) - 1
        bin_idx = np.clip(bin_idx, 0, n_bins - 1)
        bin_true[bin_idx] += t
        bin_pred[bin_idx] += p
        bin_counts[bin_idx] += 1
    nonzero = bin_counts > 0
    avg_true = np.zeros(n_bins)
    avg_pred = np.zeros(n_bins)
    avg_true[nonzero] = bin_true[nonzero] / bin_counts[nonzero]
    avg_pred[nonzero] = bin_pred[nonzero] / bin_counts[nonzero]
    plt.figure(figsize=(8, 6))
    plt.plot([0, 1], [0, 1], linestyle='--', color='gray', label='Perfect Calibration')
    plt.plot(avg_pred, avg_true, marker='o', linewidth=2, label='Calibration Curve')
    plt.xlabel('Mean Predicted Probability')
    plt.ylabel('Fraction of Positives')
    plt.title('Calibration Curve')
    plt.legend()
    plt.show()

###############################################
# 8. 主流程
###############################################
def main():
    # 設定使用校準方式：'temperature_scaling' 或 'none'
    calibration_method = 'temperature_scaling'
    
    train_file = 'data/train.csv'
    test_file = 'data/test.csv'
    
    print("讀取訓練與測試資料...")
    X_train, y_train, X_test, y_test = load_data(train_file, test_file)
    
    # 將標籤轉換為 -1 / 1
    y_train = np.where(y_train == 0, -1, 1).astype(np.int64)
    y_test = np.where(y_test == 0, -1, 1).astype(np.int64)
    
    print("對訓練資料進行標準化...")
    X_train_std, mean_train, std_train = standardize_data(X_train)
    X_test_std, _, _ = standardize_data(X_test, mean=mean_train, std=std_train)
    
    print("對訓練資料執行 PCA ...")
    X_train_pca, pca_components, pca_mean, k, _ = perform_pca(X_train_std, variance_threshold=0.8)
    print(f"PCA 完成，保留 {k} 個主成分")
    
    X_test_centered = X_test_std - pca_mean
    X_test_pca = np.dot(X_test_centered, pca_components)
    
    # -------------------------------
    # 使用 Bayesian Optimization (scikit-optimize) 來搜尋最佳 (C, gamma)
    # -------------------------------
    print("\n開始 Bayesian Optimization (F1-score) 搜尋 ...")
    best_combo, bo_results = bayes_opt_f1(X_train, y_train, n_calls=30,
                                          c_bounds=(-2, 3), gamma_bounds=(-3, 1),
                                          validation_ratio=0.2, variance_threshold=0.8)
    best_C, best_gamma = best_combo
    print("各組合 (C, gamma) F1-score:")
    for combo, f1_val in bo_results.items():
        print(f"  C={combo[0]:.4f}, gamma={combo[1]:.4f} -> F1-score: {f1_val*100:.2f}%")
    print(f"\nBayesian Optimization 選出的最佳 (C, gamma) = ({best_C}, {best_gamma})")
    
    # -------------------------------
    # 使用最佳參數在完整訓練集上重新訓練模型 (SVC)
    # -------------------------------
    print(f"\n使用最佳 (C={best_C}, gamma={best_gamma}) 重新訓練模型 ...")
    clf = SVC(C=best_C, gamma=best_gamma, kernel='rbf', probability=False)
    clf.fit(X_train_pca, y_train)
    
    y_train_pred = clf.predict(X_train_pca)
    train_acc = accuracy_score(y_train, y_train_pred)
    y_test_pred = clf.predict(X_test_pca)
    test_acc = accuracy_score(y_test, y_test_pred)
    print(f"完整訓練集 Accuracy: {train_acc*100:.2f}%，測試集 Accuracy: {test_acc*100:.2f}%")
    
    # -------------------------------
    # 使用 Temperature Scaling 校準 (LBFGS)
    # -------------------------------
    if calibration_method == 'temperature_scaling':
        print("使用 Temperature Scaling 校準...")
        T_opt = calibrate_temperature(clf, X_test_pca, y_test)
        print(f"校準完成，最佳溫度 T = {T_opt:.4f}")
        test_probs = temperature_scale_predictions(clf, X_test_pca, T_opt)
    else:
        print("不使用校準...")
        logits = clf.decision_function(X_test_pca)
        test_probs = 1.0 / (1.0 + np.exp(-logits))
    
    # -------------------------------
    # 細粒度閾值搜尋 (以 F1-score 為指標)
    # -------------------------------
    optimal_thresh, best_f1 = find_optimal_threshold_f1(test_probs, y_test)
    print(f"從測試集選出的最佳閾值為 {optimal_thresh:.3f} (F1-score: {best_f1*100:.2f}%)")
    
    # 繪製校準曲線
    y_test_binary = np.where(y_test == -1, 0, 1)
    plot_calibration_curve(test_probs, y_test_binary, n_bins=10)
    
    # -------------------------------
    # Evaluation 預測
    # -------------------------------
    eval_df = pd.read_csv("data/eval.anon.csv")
    eval_ids = pd.read_csv("data/eval.id", header=None, names=["id"])
    X_eval = eval_df.filter(regex="^x").values
    X_eval_std, _, _ = standardize_data(X_eval, mean=mean_train, std=std_train)
    X_eval_centered = X_eval_std - pca_mean
    X_eval_pca = np.dot(X_eval_centered, pca_components)
    
    if calibration_method == 'temperature_scaling':
        eval_probs = temperature_scale_predictions(clf, X_eval_pca, T_opt)
    else:
        eval_logits = clf.decision_function(X_eval_pca)
        eval_probs = 1.0 / (1.0 + np.exp(-eval_logits))
    
    eval_preds = np.where(eval_probs >= optimal_thresh, 1, -1)
    eval_preds_converted = np.where(eval_preds == -1, 0, 1)
    
    output_df = pd.DataFrame({"example_id": eval_ids["id"], "label": eval_preds_converted})
    output_folder = "SVM"
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    output_path = os.path.join(output_folder, "svm_eval_predictions.csv")
    output_df.to_csv(output_path, index=False)
    print(f"Evaluation 預測結果已寫入：{output_path}")

if __name__ == "__main__":
    main()
